//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-31968024
// Cuda compilation tools, release 12.0, V12.0.76
// Based on NVVM 7.0.1
//

.version 8.0
.target sm_52
.address_size 64

	// .globl	oxMain
.const .align 16 .b8 params[1184];

.visible .entry oxMain()
{
	.reg .pred 	%p<24>;
	.reg .b16 	%rs<10>;
	.reg .f32 	%f<126>;
	.reg .b32 	%r<32>;
	.reg .b64 	%rd<10>;


	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r3;
	mov.u32 	%r5, %ntid.y;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r8, %r6, %r5, %r7;
	ld.const.u64 	%rd2, [params+144];
	cvta.to.global.u64 	%rd3, %rd2;
	ld.const.u32 	%r9, [params+136];
	mad.lo.s32 	%r10, %r9, %r8, %r4;
	mul.wide.u32 	%rd4, %r10, 8;
	add.s64 	%rd5, %rd3, %rd4;
	add.s64 	%rd1, %rd5, 6;
	ld.global.u16 	%rs1, [%rd5+6];
	// begin inline asm
	{  cvt.f32.f16 %f25, %rs1;}

	// end inline asm
	ld.global.u16 	%rs2, [%rd5];
	// begin inline asm
	{  cvt.f32.f16 %f26, %rs2;}

	// end inline asm
	ld.global.u16 	%rs3, [%rd5+2];
	// begin inline asm
	{  cvt.f32.f16 %f27, %rs3;}

	// end inline asm
	ld.global.u16 	%rs4, [%rd5+4];
	// begin inline asm
	{  cvt.f32.f16 %f28, %rs4;}

	// end inline asm
	ld.const.u64 	%rd6, [params+160];
	cvta.to.global.u64 	%rd7, %rd6;
	ld.const.u32 	%r11, [params+152];
	mad.lo.s32 	%r12, %r11, %r8, %r4;
	cvt.u64.u32 	%rd8, %r12;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.u8 	%rs5, [%rd9];
	cvt.rn.f32.u16 	%f29, %rs5;
	div.rn.f32 	%f5, %f29, 0f437F0000;
	ld.const.f32 	%f6, [params+704];
	setp.lt.f32 	%p1, %f6, 0f00000000;
	@%p1 bra 	$L__BB0_2;
	bra.uni 	$L__BB0_1;

$L__BB0_2:
	neg.f32 	%f8, %f6;
	setp.lt.f32 	%p2, %f6, 0fBF800000;
	@%p2 bra 	$L__BB0_4;
	bra.uni 	$L__BB0_3;

$L__BB0_4:
	mul.f32 	%f35, %f6, 0fBF000000;
	cvt.rzi.f32.f32 	%f36, %f35;
	add.f32 	%f37, %f36, %f36;
	sub.f32 	%f38, %f8, %f37;
	abs.f32 	%f10, %f38;
	abs.f32 	%f11, %f5;
	setp.lt.f32 	%p3, %f11, 0f00800000;
	mul.f32 	%f39, %f11, 0f4B800000;
	selp.f32 	%f40, %f39, %f11, %p3;
	selp.f32 	%f41, 0fC3170000, 0fC2FE0000, %p3;
	mov.b32 	%r13, %f40;
	and.b32  	%r14, %r13, 8388607;
	or.b32  	%r15, %r14, 1065353216;
	mov.b32 	%f42, %r15;
	shr.u32 	%r16, %r13, 23;
	cvt.rn.f32.u32 	%f43, %r16;
	add.f32 	%f44, %f41, %f43;
	setp.gt.f32 	%p4, %f42, 0f3FB504F3;
	mul.f32 	%f45, %f42, 0f3F000000;
	add.f32 	%f46, %f44, 0f3F800000;
	selp.f32 	%f47, %f46, %f44, %p4;
	selp.f32 	%f48, %f45, %f42, %p4;
	add.f32 	%f49, %f48, 0fBF800000;
	add.f32 	%f50, %f48, 0f3F800000;
	rcp.approx.ftz.f32 	%f51, %f50;
	add.f32 	%f52, %f49, %f49;
	mul.f32 	%f53, %f52, %f51;
	mul.f32 	%f54, %f53, %f53;
	mov.f32 	%f55, 0f3C4CAF63;
	mov.f32 	%f56, 0f3B18F0FE;
	fma.rn.f32 	%f57, %f56, %f54, %f55;
	mov.f32 	%f58, 0f3DAAAABD;
	fma.rn.f32 	%f59, %f57, %f54, %f58;
	mul.rn.f32 	%f60, %f59, %f54;
	mul.rn.f32 	%f61, %f60, %f53;
	sub.f32 	%f62, %f49, %f53;
	add.f32 	%f63, %f62, %f62;
	neg.f32 	%f64, %f53;
	fma.rn.f32 	%f65, %f64, %f49, %f63;
	mul.rn.f32 	%f66, %f51, %f65;
	add.f32 	%f67, %f61, %f53;
	sub.f32 	%f68, %f53, %f67;
	add.f32 	%f69, %f61, %f68;
	add.f32 	%f70, %f66, %f69;
	add.f32 	%f71, %f67, %f70;
	sub.f32 	%f72, %f67, %f71;
	add.f32 	%f73, %f70, %f72;
	mov.f32 	%f74, 0f3F317200;
	mul.rn.f32 	%f75, %f47, %f74;
	mov.f32 	%f76, 0f35BFBE8E;
	mul.rn.f32 	%f77, %f47, %f76;
	add.f32 	%f78, %f75, %f71;
	sub.f32 	%f79, %f75, %f78;
	add.f32 	%f80, %f71, %f79;
	add.f32 	%f81, %f73, %f80;
	add.f32 	%f82, %f77, %f81;
	add.f32 	%f83, %f78, %f82;
	sub.f32 	%f84, %f78, %f83;
	add.f32 	%f85, %f82, %f84;
	abs.f32 	%f12, %f8;
	setp.gt.f32 	%p5, %f12, 0f77F684DF;
	mul.f32 	%f86, %f6, 0fB9000000;
	selp.f32 	%f87, %f86, %f8, %p5;
	mul.rn.f32 	%f88, %f87, %f83;
	neg.f32 	%f89, %f88;
	fma.rn.f32 	%f90, %f87, %f83, %f89;
	fma.rn.f32 	%f91, %f87, %f85, %f90;
	mov.f32 	%f92, 0f00000000;
	fma.rn.f32 	%f93, %f92, %f83, %f91;
	add.rn.f32 	%f94, %f88, %f93;
	neg.f32 	%f95, %f94;
	add.rn.f32 	%f96, %f88, %f95;
	add.rn.f32 	%f97, %f96, %f93;
	mov.b32 	%r17, %f94;
	setp.eq.s32 	%p6, %r17, 1118925336;
	add.s32 	%r18, %r17, -1;
	mov.b32 	%f98, %r18;
	add.f32 	%f99, %f97, 0f37000000;
	selp.f32 	%f13, %f99, %f97, %p6;
	selp.f32 	%f100, %f98, %f94, %p6;
	mov.f32 	%f101, 0f3FB8AA3B;
	mul.rn.f32 	%f102, %f100, %f101;
	cvt.rzi.f32.f32 	%f103, %f102;
	abs.f32 	%f104, %f103;
	setp.gt.f32 	%p7, %f104, 0f42FC0000;
	mov.b32 	%r19, %f103;
	and.b32  	%r20, %r19, -2147483648;
	or.b32  	%r21, %r20, 1123811328;
	mov.b32 	%f105, %r21;
	selp.f32 	%f106, %f105, %f103, %p7;
	mov.f32 	%f107, 0fBF317218;
	fma.rn.f32 	%f108, %f106, %f107, %f100;
	mov.f32 	%f109, 0f3102E308;
	fma.rn.f32 	%f110, %f106, %f109, %f108;
	mul.f32 	%f111, %f110, 0f3FB8AA3B;
	add.f32 	%f112, %f106, 0f4B40007F;
	mov.b32 	%r22, %f112;
	shl.b32 	%r23, %r22, 23;
	mov.b32 	%f113, %r23;
	ex2.approx.ftz.f32 	%f114, %f111;
	mul.f32 	%f14, %f114, %f113;
	setp.eq.f32 	%p8, %f14, 0f7F800000;
	mov.f32 	%f124, 0f7F800000;
	@%p8 bra 	$L__BB0_6;

	fma.rn.f32 	%f124, %f14, %f13, %f14;

$L__BB0_6:
	setp.neu.f32 	%p9, %f5, 0f00000000;
	@%p9 bra 	$L__BB0_8;

	setp.eq.f32 	%p10, %f10, 0f3F800000;
	add.f32 	%f115, %f5, %f5;
	mov.b32 	%r24, %f115;
	selp.b32 	%r25, %r24, 0, %p10;
	or.b32  	%r26, %r25, 2139095040;
	setp.gt.f32 	%p11, %f6, 0f80000000;
	selp.b32 	%r27, %r26, %r25, %p11;
	mov.b32 	%f124, %r27;

$L__BB0_8:
	add.f32 	%f116, %f11, %f12;
	mov.b32 	%r28, %f116;
	setp.lt.s32 	%p12, %r28, 2139095040;
	@%p12 bra 	$L__BB0_15;

	setp.gtu.f32 	%p13, %f11, 0f7F800000;
	setp.gtu.f32 	%p14, %f12, 0f7F800000;
	or.pred  	%p15, %p13, %p14;
	@%p15 bra 	$L__BB0_14;
	bra.uni 	$L__BB0_10;

$L__BB0_14:
	sub.f32 	%f124, %f5, %f6;
	bra.uni 	$L__BB0_15;

$L__BB0_1:
	add.f32 	%f30, %f5, 0fBF800000;
	fma.rn.f32 	%f31, %f6, %f30, 0f3F800000;
	cvt.sat.f32.f32 	%f125, %f31;
	bra.uni 	$L__BB0_16;

$L__BB0_3:
	add.f32 	%f32, %f5, 0fBF800000;
	fma.rn.f32 	%f33, %f32, %f8, 0f3F800000;
	cvt.sat.f32.f32 	%f125, %f33;
	bra.uni 	$L__BB0_16;

$L__BB0_10:
	setp.eq.f32 	%p16, %f12, 0f7F800000;
	@%p16 bra 	$L__BB0_13;
	bra.uni 	$L__BB0_11;

$L__BB0_13:
	setp.gt.f32 	%p19, %f11, 0f3F800000;
	selp.b32 	%r29, 2139095040, 0, %p19;
	xor.b32  	%r30, %r29, 2139095040;
	setp.gt.f32 	%p20, %f6, 0f80000000;
	selp.b32 	%r31, %r30, %r29, %p20;
	mov.b32 	%f124, %r31;
	bra.uni 	$L__BB0_15;

$L__BB0_11:
	setp.neu.f32 	%p17, %f11, 0f7F800000;
	@%p17 bra 	$L__BB0_15;

	setp.le.f32 	%p18, %f6, 0f80000000;
	selp.f32 	%f124, 0f7F800000, 0f00000000, %p18;

$L__BB0_15:
	setp.eq.f32 	%p21, %f6, 0f80000000;
	setp.eq.f32 	%p22, %f5, 0f3F800000;
	or.pred  	%p23, %p21, %p22;
	selp.f32 	%f117, 0f3F800000, %f124, %p23;
	cvt.sat.f32.f32 	%f125, %f117;

$L__BB0_16:
	mul.f32 	%f118, %f26, %f125;
	mul.f32 	%f119, %f27, %f125;
	mul.f32 	%f120, %f28, %f125;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs8, %f120;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs7, %f119;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs6, %f118;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs9, %f25;}

	// end inline asm
	st.global.v4.u16 	[%rd1+-6], {%rs6, %rs7, %rs8, %rs9};
	ret;

}

