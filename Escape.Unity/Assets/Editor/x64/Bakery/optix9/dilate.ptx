//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-31968024
// Cuda compilation tools, release 12.0, V12.0.76
// Based on NVVM 7.0.1
//

.version 8.0
.target sm_52
.address_size 64

	// .globl	oxMain
.const .align 16 .b8 params[1184];

.visible .entry oxMain()
{
	.reg .pred 	%p<14>;
	.reg .b16 	%rs<45>;
	.reg .f32 	%f<158>;
	.reg .b32 	%r<59>;
	.reg .b64 	%rd<27>;


	mov.u32 	%r25, %ctaid.x;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r26, %tid.x;
	mad.lo.s32 	%r2, %r25, %r1, %r26;
	mov.u32 	%r3, %ntid.y;
	mov.u32 	%r27, %ctaid.y;
	mov.u32 	%r28, %tid.y;
	mad.lo.s32 	%r4, %r27, %r3, %r28;
	ld.const.u64 	%rd10, [params+144];
	cvta.to.global.u64 	%rd1, %rd10;
	ld.const.u32 	%r5, [params+136];
	mul.lo.s32 	%r6, %r5, %r4;
	add.s32 	%r29, %r6, %r2;
	mul.wide.u32 	%rd11, %r29, 4;
	add.s64 	%rd12, %rd1, %rd11;
	add.s64 	%rd2, %rd12, 3;
	ld.global.u8 	%rs8, [%rd12+3];
	setp.ne.s16 	%p1, %rs8, 0;
	@%p1 bra 	$L__BB0_17;

	mov.u32 	%r53, 0;
	ld.global.v4.u8 	{%rs9, %rs10, %rs11, %rs12}, [%rd2+-3];
	cvt.rn.f32.u16 	%f61, %rs11;
	div.rn.f32 	%f134, %f61, 0f437F0000;
	cvt.rn.f32.u16 	%f62, %rs10;
	div.rn.f32 	%f135, %f62, 0f437F0000;
	cvt.rn.f32.u16 	%f63, %rs9;
	div.rn.f32 	%f136, %f63, 0f437F0000;
	add.s32 	%r31, %r2, -1;
	setp.eq.s32 	%p2, %r2, 0;
	selp.b32 	%r7, 0, %r31, %p2;
	add.s32 	%r32, %r4, -1;
	setp.eq.s32 	%p3, %r4, 0;
	selp.b32 	%r33, 0, %r32, %p3;
	mov.u32 	%r34, %nctaid.x;
	mad.lo.s32 	%r35, %r34, %r1, -1;
	setp.eq.s32 	%p4, %r2, %r35;
	add.s32 	%r36, %r2, 1;
	selp.b32 	%r8, %r35, %r36, %p4;
	mov.u32 	%r37, %nctaid.y;
	mad.lo.s32 	%r38, %r37, %r3, -1;
	setp.eq.s32 	%p5, %r4, %r38;
	add.s32 	%r39, %r4, 1;
	selp.b32 	%r9, %r38, %r39, %p5;
	mul.lo.s32 	%r10, %r5, %r33;
	add.s32 	%r40, %r10, %r7;
	mul.wide.u32 	%rd13, %r40, 4;
	add.s64 	%rd14, %rd1, %rd13;
	add.s64 	%rd3, %rd14, 3;
	ld.global.u8 	%rs1, [%rd14+3];
	setp.eq.s16 	%p6, %rs1, 0;
	mov.f32 	%f137, 0f00000000;
	@%p6 bra 	$L__BB0_3;

	mov.u32 	%r53, 1;
	ld.global.v4.u8 	{%rs13, %rs14, %rs15, %rs16}, [%rd3+-3];
	cvt.rn.f32.u16 	%f64, %rs15;
	div.rn.f32 	%f65, %f64, 0f437F0000;
	cvt.rn.f32.u16 	%f66, %rs14;
	div.rn.f32 	%f67, %f66, 0f437F0000;
	cvt.rn.f32.u16 	%f68, %rs13;
	div.rn.f32 	%f69, %f68, 0f437F0000;
	cvt.rn.f32.u16 	%f70, %rs1;
	div.rn.f32 	%f71, %f70, 0f437F0000;
	add.f32 	%f134, %f134, %f65;
	add.f32 	%f135, %f135, %f67;
	add.f32 	%f136, %f136, %f69;
	add.f32 	%f137, %f71, 0f00000000;

$L__BB0_3:
	add.s32 	%r42, %r10, %r2;
	mul.wide.u32 	%rd15, %r42, 4;
	add.s64 	%rd16, %rd1, %rd15;
	add.s64 	%rd4, %rd16, 3;
	ld.global.u8 	%rs2, [%rd16+3];
	setp.eq.s16 	%p7, %rs2, 0;
	@%p7 bra 	$L__BB0_5;

	ld.global.v4.u8 	{%rs17, %rs18, %rs19, %rs20}, [%rd4+-3];
	cvt.rn.f32.u16 	%f72, %rs19;
	div.rn.f32 	%f73, %f72, 0f437F0000;
	cvt.rn.f32.u16 	%f74, %rs18;
	div.rn.f32 	%f75, %f74, 0f437F0000;
	cvt.rn.f32.u16 	%f76, %rs17;
	div.rn.f32 	%f77, %f76, 0f437F0000;
	cvt.rn.f32.u16 	%f78, %rs2;
	div.rn.f32 	%f79, %f78, 0f437F0000;
	add.f32 	%f134, %f134, %f73;
	add.f32 	%f135, %f135, %f75;
	add.f32 	%f136, %f136, %f77;
	add.f32 	%f137, %f137, %f79;
	add.s32 	%r53, %r53, 1;

$L__BB0_5:
	add.s32 	%r43, %r10, %r8;
	mul.wide.u32 	%rd17, %r43, 4;
	add.s64 	%rd18, %rd1, %rd17;
	add.s64 	%rd5, %rd18, 3;
	ld.global.u8 	%rs3, [%rd18+3];
	setp.eq.s16 	%p8, %rs3, 0;
	@%p8 bra 	$L__BB0_7;

	ld.global.v4.u8 	{%rs21, %rs22, %rs23, %rs24}, [%rd5+-3];
	cvt.rn.f32.u16 	%f80, %rs23;
	div.rn.f32 	%f81, %f80, 0f437F0000;
	cvt.rn.f32.u16 	%f82, %rs22;
	div.rn.f32 	%f83, %f82, 0f437F0000;
	cvt.rn.f32.u16 	%f84, %rs21;
	div.rn.f32 	%f85, %f84, 0f437F0000;
	cvt.rn.f32.u16 	%f86, %rs3;
	div.rn.f32 	%f87, %f86, 0f437F0000;
	add.f32 	%f134, %f134, %f81;
	add.f32 	%f135, %f135, %f83;
	add.f32 	%f136, %f136, %f85;
	add.f32 	%f137, %f137, %f87;
	add.s32 	%r53, %r53, 1;

$L__BB0_7:
	add.s32 	%r44, %r6, %r7;
	mul.wide.u32 	%rd19, %r44, 4;
	add.s64 	%rd20, %rd1, %rd19;
	add.s64 	%rd6, %rd20, 3;
	ld.global.u8 	%rs4, [%rd20+3];
	setp.eq.s16 	%p9, %rs4, 0;
	@%p9 bra 	$L__BB0_9;

	ld.global.v4.u8 	{%rs25, %rs26, %rs27, %rs28}, [%rd6+-3];
	cvt.rn.f32.u16 	%f88, %rs27;
	div.rn.f32 	%f89, %f88, 0f437F0000;
	cvt.rn.f32.u16 	%f90, %rs26;
	div.rn.f32 	%f91, %f90, 0f437F0000;
	cvt.rn.f32.u16 	%f92, %rs25;
	div.rn.f32 	%f93, %f92, 0f437F0000;
	cvt.rn.f32.u16 	%f94, %rs4;
	div.rn.f32 	%f95, %f94, 0f437F0000;
	add.f32 	%f134, %f134, %f89;
	add.f32 	%f135, %f135, %f91;
	add.f32 	%f136, %f136, %f93;
	add.f32 	%f137, %f137, %f95;
	add.s32 	%r53, %r53, 1;

$L__BB0_9:
	add.s32 	%r45, %r6, %r8;
	mul.wide.u32 	%rd21, %r45, 4;
	add.s64 	%rd22, %rd1, %rd21;
	add.s64 	%rd7, %rd22, 3;
	ld.global.u8 	%rs5, [%rd22+3];
	setp.eq.s16 	%p10, %rs5, 0;
	@%p10 bra 	$L__BB0_11;

	ld.global.v4.u8 	{%rs29, %rs30, %rs31, %rs32}, [%rd7+-3];
	cvt.rn.f32.u16 	%f96, %rs31;
	div.rn.f32 	%f97, %f96, 0f437F0000;
	cvt.rn.f32.u16 	%f98, %rs30;
	div.rn.f32 	%f99, %f98, 0f437F0000;
	cvt.rn.f32.u16 	%f100, %rs29;
	div.rn.f32 	%f101, %f100, 0f437F0000;
	cvt.rn.f32.u16 	%f102, %rs5;
	div.rn.f32 	%f103, %f102, 0f437F0000;
	add.f32 	%f134, %f134, %f97;
	add.f32 	%f135, %f135, %f99;
	add.f32 	%f136, %f136, %f101;
	add.f32 	%f137, %f137, %f103;
	add.s32 	%r53, %r53, 1;

$L__BB0_11:
	mul.lo.s32 	%r20, %r5, %r9;
	add.s32 	%r46, %r20, %r7;
	mul.wide.u32 	%rd23, %r46, 4;
	add.s64 	%rd24, %rd1, %rd23;
	add.s64 	%rd8, %rd24, 3;
	ld.global.u8 	%rs6, [%rd24+3];
	setp.eq.s16 	%p11, %rs6, 0;
	@%p11 bra 	$L__BB0_13;

	ld.global.v4.u8 	{%rs33, %rs34, %rs35, %rs36}, [%rd8+-3];
	cvt.rn.f32.u16 	%f104, %rs35;
	div.rn.f32 	%f105, %f104, 0f437F0000;
	cvt.rn.f32.u16 	%f106, %rs34;
	div.rn.f32 	%f107, %f106, 0f437F0000;
	cvt.rn.f32.u16 	%f108, %rs33;
	div.rn.f32 	%f109, %f108, 0f437F0000;
	cvt.rn.f32.u16 	%f110, %rs6;
	div.rn.f32 	%f111, %f110, 0f437F0000;
	add.f32 	%f134, %f134, %f105;
	add.f32 	%f135, %f135, %f107;
	add.f32 	%f136, %f136, %f109;
	add.f32 	%f137, %f137, %f111;
	add.s32 	%r53, %r53, 1;

$L__BB0_13:
	add.s32 	%r47, %r20, %r8;
	mul.wide.u32 	%rd25, %r47, 4;
	add.s64 	%rd26, %rd1, %rd25;
	add.s64 	%rd9, %rd26, 3;
	ld.global.u8 	%rs7, [%rd26+3];
	setp.eq.s16 	%p12, %rs7, 0;
	@%p12 bra 	$L__BB0_15;

	ld.global.v4.u8 	{%rs37, %rs38, %rs39, %rs40}, [%rd9+-3];
	cvt.rn.f32.u16 	%f112, %rs39;
	div.rn.f32 	%f113, %f112, 0f437F0000;
	cvt.rn.f32.u16 	%f114, %rs38;
	div.rn.f32 	%f115, %f114, 0f437F0000;
	cvt.rn.f32.u16 	%f116, %rs37;
	div.rn.f32 	%f117, %f116, 0f437F0000;
	cvt.rn.f32.u16 	%f118, %rs7;
	div.rn.f32 	%f119, %f118, 0f437F0000;
	add.f32 	%f134, %f134, %f113;
	add.f32 	%f135, %f135, %f115;
	add.f32 	%f136, %f136, %f117;
	add.f32 	%f137, %f137, %f119;
	add.s32 	%r53, %r53, 1;

$L__BB0_15:
	setp.eq.s32 	%p13, %r53, 0;
	@%p13 bra 	$L__BB0_17;

	cvt.rn.f32.u32 	%f120, %r53;
	rcp.rn.f32 	%f121, %f120;
	mul.f32 	%f122, %f134, %f121;
	mul.f32 	%f123, %f135, %f121;
	mul.f32 	%f124, %f136, %f121;
	mul.f32 	%f125, %f137, %f121;
	mul.f32 	%f126, %f122, 0f437F0000;
	mul.f32 	%f127, %f123, 0f437F0000;
	mul.f32 	%f128, %f124, 0f437F0000;
	mul.f32 	%f129, %f125, 0f437F0000;
	cvt.rzi.u32.f32 	%r48, %f128;
	cvt.rzi.u32.f32 	%r49, %f127;
	cvt.rzi.u32.f32 	%r50, %f126;
	cvt.rzi.u32.f32 	%r51, %f129;
	cvt.u16.u32 	%rs41, %r51;
	cvt.u16.u32 	%rs42, %r50;
	cvt.u16.u32 	%rs43, %r49;
	cvt.u16.u32 	%rs44, %r48;
	st.global.v4.u8 	[%rd2+-3], {%rs44, %rs43, %rs42, %rs41};

$L__BB0_17:
	ret;

}

