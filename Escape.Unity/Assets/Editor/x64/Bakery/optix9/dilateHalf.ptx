//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-31968024
// Cuda compilation tools, release 12.0, V12.0.76
// Based on NVVM 7.0.1
//

.version 8.0
.target sm_52
.address_size 64

	// .globl	oxMain
.const .align 16 .b8 params[1184];

.visible .entry oxMain()
{
	.reg .pred 	%p<14>;
	.reg .b16 	%rs<45>;
	.reg .f32 	%f<105>;
	.reg .b32 	%r<55>;
	.reg .b64 	%rd<27>;


	mov.u32 	%r25, %ctaid.x;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r26, %tid.x;
	mad.lo.s32 	%r2, %r25, %r1, %r26;
	mov.u32 	%r3, %ntid.y;
	mov.u32 	%r27, %ctaid.y;
	mov.u32 	%r28, %tid.y;
	mad.lo.s32 	%r4, %r27, %r3, %r28;
	ld.const.u64 	%rd10, [params+144];
	cvta.to.global.u64 	%rd1, %rd10;
	ld.const.u32 	%r5, [params+136];
	mul.lo.s32 	%r6, %r5, %r4;
	add.s32 	%r29, %r6, %r2;
	mul.wide.u32 	%rd11, %r29, 8;
	add.s64 	%rd12, %rd1, %rd11;
	add.s64 	%rd2, %rd12, 6;
	ld.global.u16 	%rs2, [%rd12+6];
	mov.f32 	%f46, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f46;}

	// end inline asm
	setp.gt.u16 	%p1, %rs2, %rs1;
	@%p1 bra 	$L__BB0_17;

	ld.global.u16 	%rs3, [%rd2+-6];
	// begin inline asm
	{  cvt.f32.f16 %f87, %rs3;}

	// end inline asm
	ld.global.u16 	%rs4, [%rd2+-4];
	// begin inline asm
	{  cvt.f32.f16 %f88, %rs4;}

	// end inline asm
	ld.global.u16 	%rs5, [%rd2+-2];
	// begin inline asm
	{  cvt.f32.f16 %f89, %rs5;}

	// end inline asm
	add.s32 	%r31, %r2, -1;
	setp.eq.s32 	%p2, %r2, 0;
	mov.u32 	%r49, 0;
	selp.b32 	%r7, 0, %r31, %p2;
	add.s32 	%r32, %r4, -1;
	setp.eq.s32 	%p3, %r4, 0;
	selp.b32 	%r33, 0, %r32, %p3;
	mov.u32 	%r34, %nctaid.x;
	mad.lo.s32 	%r35, %r34, %r1, -1;
	setp.eq.s32 	%p4, %r2, %r35;
	add.s32 	%r36, %r2, 1;
	selp.b32 	%r8, %r35, %r36, %p4;
	mov.u32 	%r37, %nctaid.y;
	mad.lo.s32 	%r38, %r37, %r3, -1;
	setp.eq.s32 	%p5, %r4, %r38;
	add.s32 	%r39, %r4, 1;
	selp.b32 	%r9, %r38, %r39, %p5;
	mul.lo.s32 	%r10, %r5, %r33;
	add.s32 	%r40, %r10, %r7;
	mul.wide.u32 	%rd13, %r40, 8;
	add.s64 	%rd14, %rd1, %rd13;
	add.s64 	%rd3, %rd14, 6;
	ld.global.u16 	%rs7, [%rd14+6];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs6, %f46;}

	// end inline asm
	setp.le.u16 	%p6, %rs7, %rs6;
	@%p6 bra 	$L__BB0_3;

	ld.global.u16 	%rs8, [%rd3+-6];
	// begin inline asm
	{  cvt.f32.f16 %f51, %rs8;}

	// end inline asm
	ld.global.u16 	%rs9, [%rd3+-4];
	// begin inline asm
	{  cvt.f32.f16 %f52, %rs9;}

	// end inline asm
	ld.global.u16 	%rs10, [%rd3+-2];
	// begin inline asm
	{  cvt.f32.f16 %f53, %rs10;}

	// end inline asm
	add.f32 	%f87, %f87, %f51;
	add.f32 	%f88, %f88, %f52;
	add.f32 	%f89, %f89, %f53;
	mov.u32 	%r49, 1;

$L__BB0_3:
	add.s32 	%r42, %r10, %r2;
	mul.wide.u32 	%rd15, %r42, 8;
	add.s64 	%rd16, %rd1, %rd15;
	add.s64 	%rd4, %rd16, 6;
	ld.global.u16 	%rs12, [%rd16+6];
	mov.f32 	%f54, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs11, %f54;}

	// end inline asm
	setp.le.u16 	%p7, %rs12, %rs11;
	@%p7 bra 	$L__BB0_5;

	ld.global.u16 	%rs13, [%rd4+-6];
	// begin inline asm
	{  cvt.f32.f16 %f55, %rs13;}

	// end inline asm
	ld.global.u16 	%rs14, [%rd4+-4];
	// begin inline asm
	{  cvt.f32.f16 %f56, %rs14;}

	// end inline asm
	ld.global.u16 	%rs15, [%rd4+-2];
	// begin inline asm
	{  cvt.f32.f16 %f57, %rs15;}

	// end inline asm
	add.f32 	%f87, %f87, %f55;
	add.f32 	%f88, %f88, %f56;
	add.f32 	%f89, %f89, %f57;
	add.s32 	%r49, %r49, 1;

$L__BB0_5:
	add.s32 	%r43, %r10, %r8;
	mul.wide.u32 	%rd17, %r43, 8;
	add.s64 	%rd18, %rd1, %rd17;
	add.s64 	%rd5, %rd18, 6;
	ld.global.u16 	%rs17, [%rd18+6];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs16, %f54;}

	// end inline asm
	setp.le.u16 	%p8, %rs17, %rs16;
	@%p8 bra 	$L__BB0_7;

	ld.global.u16 	%rs18, [%rd5+-6];
	// begin inline asm
	{  cvt.f32.f16 %f59, %rs18;}

	// end inline asm
	ld.global.u16 	%rs19, [%rd5+-4];
	// begin inline asm
	{  cvt.f32.f16 %f60, %rs19;}

	// end inline asm
	ld.global.u16 	%rs20, [%rd5+-2];
	// begin inline asm
	{  cvt.f32.f16 %f61, %rs20;}

	// end inline asm
	add.f32 	%f87, %f87, %f59;
	add.f32 	%f88, %f88, %f60;
	add.f32 	%f89, %f89, %f61;
	add.s32 	%r49, %r49, 1;

$L__BB0_7:
	add.s32 	%r44, %r6, %r7;
	mul.wide.u32 	%rd19, %r44, 8;
	add.s64 	%rd20, %rd1, %rd19;
	add.s64 	%rd6, %rd20, 6;
	ld.global.u16 	%rs22, [%rd20+6];
	mov.f32 	%f62, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs21, %f62;}

	// end inline asm
	setp.le.u16 	%p9, %rs22, %rs21;
	@%p9 bra 	$L__BB0_9;

	ld.global.u16 	%rs23, [%rd6+-6];
	// begin inline asm
	{  cvt.f32.f16 %f63, %rs23;}

	// end inline asm
	ld.global.u16 	%rs24, [%rd6+-4];
	// begin inline asm
	{  cvt.f32.f16 %f64, %rs24;}

	// end inline asm
	ld.global.u16 	%rs25, [%rd6+-2];
	// begin inline asm
	{  cvt.f32.f16 %f65, %rs25;}

	// end inline asm
	add.f32 	%f87, %f87, %f63;
	add.f32 	%f88, %f88, %f64;
	add.f32 	%f89, %f89, %f65;
	add.s32 	%r49, %r49, 1;

$L__BB0_9:
	add.s32 	%r45, %r6, %r8;
	mul.wide.u32 	%rd21, %r45, 8;
	add.s64 	%rd22, %rd1, %rd21;
	add.s64 	%rd7, %rd22, 6;
	ld.global.u16 	%rs27, [%rd22+6];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs26, %f62;}

	// end inline asm
	setp.le.u16 	%p10, %rs27, %rs26;
	@%p10 bra 	$L__BB0_11;

	ld.global.u16 	%rs28, [%rd7+-6];
	// begin inline asm
	{  cvt.f32.f16 %f67, %rs28;}

	// end inline asm
	ld.global.u16 	%rs29, [%rd7+-4];
	// begin inline asm
	{  cvt.f32.f16 %f68, %rs29;}

	// end inline asm
	ld.global.u16 	%rs30, [%rd7+-2];
	// begin inline asm
	{  cvt.f32.f16 %f69, %rs30;}

	// end inline asm
	add.f32 	%f87, %f87, %f67;
	add.f32 	%f88, %f88, %f68;
	add.f32 	%f89, %f89, %f69;
	add.s32 	%r49, %r49, 1;

$L__BB0_11:
	mul.lo.s32 	%r20, %r5, %r9;
	add.s32 	%r46, %r20, %r7;
	mul.wide.u32 	%rd23, %r46, 8;
	add.s64 	%rd24, %rd1, %rd23;
	add.s64 	%rd8, %rd24, 6;
	ld.global.u16 	%rs32, [%rd24+6];
	mov.f32 	%f70, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs31, %f70;}

	// end inline asm
	setp.le.u16 	%p11, %rs32, %rs31;
	@%p11 bra 	$L__BB0_13;

	ld.global.u16 	%rs33, [%rd8+-6];
	// begin inline asm
	{  cvt.f32.f16 %f71, %rs33;}

	// end inline asm
	ld.global.u16 	%rs34, [%rd8+-4];
	// begin inline asm
	{  cvt.f32.f16 %f72, %rs34;}

	// end inline asm
	ld.global.u16 	%rs35, [%rd8+-2];
	// begin inline asm
	{  cvt.f32.f16 %f73, %rs35;}

	// end inline asm
	add.f32 	%f87, %f87, %f71;
	add.f32 	%f88, %f88, %f72;
	add.f32 	%f89, %f89, %f73;
	add.s32 	%r49, %r49, 1;

$L__BB0_13:
	add.s32 	%r47, %r20, %r8;
	mul.wide.u32 	%rd25, %r47, 8;
	add.s64 	%rd26, %rd1, %rd25;
	add.s64 	%rd9, %rd26, 6;
	ld.global.u16 	%rs37, [%rd26+6];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs36, %f70;}

	// end inline asm
	setp.le.u16 	%p12, %rs37, %rs36;
	@%p12 bra 	$L__BB0_15;

	ld.global.u16 	%rs38, [%rd9+-6];
	// begin inline asm
	{  cvt.f32.f16 %f75, %rs38;}

	// end inline asm
	ld.global.u16 	%rs39, [%rd9+-4];
	// begin inline asm
	{  cvt.f32.f16 %f76, %rs39;}

	// end inline asm
	ld.global.u16 	%rs40, [%rd9+-2];
	// begin inline asm
	{  cvt.f32.f16 %f77, %rs40;}

	// end inline asm
	add.f32 	%f87, %f87, %f75;
	add.f32 	%f88, %f88, %f76;
	add.f32 	%f89, %f89, %f77;
	add.s32 	%r49, %r49, 1;

$L__BB0_15:
	setp.eq.s32 	%p13, %r49, 0;
	@%p13 bra 	$L__BB0_17;

	cvt.rn.f32.u32 	%f82, %r49;
	rcp.rn.f32 	%f83, %f82;
	mov.f32 	%f81, 0f3F800000;
	mul.f32 	%f78, %f87, %f83;
	mul.f32 	%f79, %f88, %f83;
	mul.f32 	%f80, %f89, %f83;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs43, %f80;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs42, %f79;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs41, %f78;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs44, %f81;}

	// end inline asm
	st.global.v4.u16 	[%rd2+-6], {%rs41, %rs42, %rs43, %rs44};

$L__BB0_17:
	ret;

}

